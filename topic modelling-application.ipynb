{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5093a966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (2.5.1+cu121)\n",
      "Requirement already satisfied: transformers in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (4.52.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (2.0.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (3.10.0)\n",
      "Requirement already satisfied: plotly in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (6.1.0)\n",
      "Requirement already satisfied: spacy in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (3.8.6)\n",
      "Requirement already satisfied: filelock in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from torch) (80.7.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from transformers) (0.31.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from matplotlib) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from matplotlib) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: narwhals>=1.15.1 in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from plotly) (1.40.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from spacy) (1.0.12)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from spacy) (8.3.6)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from spacy) (0.9.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from spacy) (2.10.3)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from requests->transformers) (2025.4.26)\n",
      "Requirement already satisfied: blis<1.4.0,>=1.3.0 in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.1)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.0)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Collecting nl-core-news-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/nl_core_news_sm-3.8.0/nl_core_news_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     - -------------------------------------- 0.5/12.8 MB 3.4 MB/s eta 0:00:04\n",
      "     ---- ----------------------------------- 1.3/12.8 MB 3.4 MB/s eta 0:00:04\n",
      "     ------ --------------------------------- 2.1/12.8 MB 3.4 MB/s eta 0:00:04\n",
      "     -------- ------------------------------- 2.6/12.8 MB 3.4 MB/s eta 0:00:03\n",
      "     ---------- ----------------------------- 3.4/12.8 MB 3.5 MB/s eta 0:00:03\n",
      "     ------------- -------------------------- 4.5/12.8 MB 3.6 MB/s eta 0:00:03\n",
      "     ---------------- ----------------------- 5.2/12.8 MB 3.7 MB/s eta 0:00:03\n",
      "     ------------------ --------------------- 6.0/12.8 MB 3.8 MB/s eta 0:00:02\n",
      "     --------------------- ------------------ 6.8/12.8 MB 3.7 MB/s eta 0:00:02\n",
      "     ---------------------- ----------------- 7.3/12.8 MB 3.7 MB/s eta 0:00:02\n",
      "     ------------------------- -------------- 8.1/12.8 MB 3.7 MB/s eta 0:00:02\n",
      "     --------------------------- ------------ 8.9/12.8 MB 3.7 MB/s eta 0:00:02\n",
      "     ------------------------------ --------- 9.7/12.8 MB 3.7 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 10.7/12.8 MB 3.8 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 11.5/12.8 MB 3.8 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 12.3/12.8 MB 3.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 12.8/12.8 MB 3.8 MB/s eta 0:00:00\n",
      "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('nl_core_news_sm')\n",
      "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
      "Requirement already satisfied: torch in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (2.5.1+cu121)\n",
      "Requirement already satisfied: torchvision in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (0.20.1+cu121)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (2.5.1+cu121)\n",
      "Requirement already satisfied: filelock in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from torch) (80.7.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from torchvision) (2.0.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from torchvision) (11.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from jinja2->torch) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch transformers scikit-learn pandas numpy matplotlib plotly spacy\n",
    "!python -m spacy download nl_core_news_sm\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d1af545",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{np.int64(0): 'Cultuur_en_sport',\n",
       " np.int64(1): 'Ramp_ongeval',\n",
       " np.int64(2): 'arbeid',\n",
       " np.int64(3): 'buitenlandse_zaken',\n",
       " np.int64(4): 'burgerrechten',\n",
       " np.int64(5): 'criminaliteit',\n",
       " np.int64(6): 'democratie_en_bestuur',\n",
       " np.int64(7): 'gezondheidszorg',\n",
       " np.int64(8): 'huisvesting',\n",
       " np.int64(9): 'immigratie',\n",
       " np.int64(10): 'milieu',\n",
       " np.int64(11): 'not-matched',\n",
       " np.int64(12): 'onderwijs',\n",
       " np.int64(13): 'religie',\n",
       " np.int64(14): 'ruimtelijke_ordening',\n",
       " np.int64(15): 'sociale_zaken'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'label'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Home\\anaconda3\\envs\\policy\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3804\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3805\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._engine.get_loc(casted_key)\n\u001b[32m   3806\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'label'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 82\u001b[39m\n\u001b[32m     74\u001b[39m results_df = pd.DataFrame({\n\u001b[32m     75\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mdocument\u001b[39m\u001b[33m'\u001b[39m: doc_ids,\n\u001b[32m     76\u001b[39m     \u001b[33m'\u001b[39m\u001b[33msentence\u001b[39m\u001b[33m'\u001b[39m: all_sentences,\n\u001b[32m     77\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mlabels\u001b[39m\u001b[33m'\u001b[39m : [r[\u001b[33m'\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m results],\n\u001b[32m     78\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mscores\u001b[39m\u001b[33m'\u001b[39m: [r[\u001b[33m'\u001b[39m\u001b[33mscore\u001b[39m\u001b[33m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m results]\n\u001b[32m     79\u001b[39m })\n\u001b[32m     81\u001b[39m \u001b[38;5;66;03m# Label int and topic only for confident predictions\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m results_df[\u001b[33m'\u001b[39m\u001b[33mlabel_int\u001b[39m\u001b[33m'\u001b[39m] = results_df[\u001b[33m'\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m'\u001b[39m].apply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mint\u001b[39m(x.replace(\u001b[33m'\u001b[39m\u001b[33mLABEL_\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m)) \u001b[38;5;28;01mif\u001b[39;00m pd.notnull(x) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m     83\u001b[39m results_df[\u001b[33m'\u001b[39m\u001b[33mtopic\u001b[39m\u001b[33m'\u001b[39m] = results_df[\u001b[33m'\u001b[39m\u001b[33mlabel_int\u001b[39m\u001b[33m'\u001b[39m].map(label2topic)\n\u001b[32m     85\u001b[39m \u001b[38;5;66;03m# Embedding function (only for sentences with valid label)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Home\\anaconda3\\envs\\policy\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4100\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4101\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4102\u001b[39m indexer = \u001b[38;5;28mself\u001b[39m.columns.get_loc(key)\n\u001b[32m   4103\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4104\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Home\\anaconda3\\envs\\policy\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3807\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3808\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3809\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3810\u001b[39m     ):\n\u001b[32m   3811\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3814\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3815\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3816\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3817\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'label'"
     ]
    }
   ],
   "source": [
    "#policy_Embedder\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline, AutoModel\n",
    "import pickle\n",
    "\n",
    "# Load dictionary\n",
    "dictionary_df = pd.read_excel('policy_dictionairy.xlsx')\n",
    "\n",
    "# Load policy documents\n",
    "unseen_policy_df = pd.read_excel(\"Policy-documents/2015_selectedtypes_cleaned (1).xlsx\")\n",
    "nlp = spacy.load(\"nl_core_news_sm\")\n",
    "\n",
    "# Load topic mappings\n",
    "with open('policy-label2topic.pickle', 'rb') as fp:\n",
    "    label2topic = pickle.load(fp)\n",
    "with open('policy-topic2label.pickle', 'rb') as fp:\n",
    "    topic2label = pickle.load(fp)\n",
    "\n",
    "display(label2topic)\n",
    "\n",
    "# Device detection\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "MODEL_DIR = \"bertje_policy/checkpoint-63\"\n",
    "BASE_MODEL = \"GroNLP/bert-base-dutch-cased\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_DIR).to(device)\n",
    "embedder = AutoModel.from_pretrained(BASE_MODEL).to(device)\n",
    "\n",
    "def split_sentences(text):\n",
    "    doc = nlp(str(text))\n",
    "    return [sent.text.strip() for sent in doc.sents]\n",
    "\n",
    "# Collect all sentences and doc_ids\n",
    "all_sentences = []\n",
    "doc_ids = []\n",
    "for i, row in unseen_policy_df.iterrows():\n",
    "    doc_id = row['filename'] if 'filename' in row else f'doc_{i}'\n",
    "    text = row['clean_text']\n",
    "    if not pd.isna(text):\n",
    "        sents = split_sentences(text)\n",
    "        all_sentences.extend(sents)\n",
    "        doc_ids.extend([doc_id]*len(sents))\n",
    "\n",
    "# HuggingFace pipeline for topic prediction\n",
    "nlp_pipe = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=0 if torch.cuda.is_available() else -1,\n",
    ")\n",
    "\n",
    "# Predict topics (batched)\n",
    "results = nlp_pipe(all_sentences, truncation=True, max_length=128, batch_size=32)\n",
    "\n",
    "# Keep results and sentences aligned; fill label/score only if score >= 0.7\n",
    "#labels = []\n",
    "#cores = []\n",
    "#for r in results:\n",
    "#    if r['score'] >= 0.3:\n",
    "#        labels.append(r['label'])\n",
    "#        scores.append(r['score'])\n",
    "#    else:\n",
    "#        labels.append(None)\n",
    "#        scores.append(None)\n",
    "\n",
    "# DataFrame build\n",
    "results_df = pd.DataFrame({\n",
    "    'document': doc_ids,\n",
    "    'sentence': all_sentences,\n",
    "    'labels' : [r['label'] for r in results],\n",
    "    'scores': [r['score'] for r in results]\n",
    "})\n",
    "\n",
    "# Label int and topic only for confident predictions\n",
    "results_df['label_int'] = results_df['label'].apply(lambda x: int(x.replace('LABEL_', '')) if pd.notnull(x) else None)\n",
    "results_df['topic'] = results_df['label_int'].map(label2topic)\n",
    "\n",
    "# Embedding function (only for sentences with valid label)\n",
    "def bertje_embed(sentences, tokenizer, model, device=device, batch_size=32):\n",
    "    embeddings = []\n",
    "    for i in range(0, len(sentences), batch_size):\n",
    "        batch = sentences[i:i+batch_size]\n",
    "        encoded = tokenizer(batch, return_tensors='pt', padding=True, truncation=True, max_length=128).to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(**encoded)\n",
    "        batch_embeds = output.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "        embeddings.extend(batch_embeds)\n",
    "    return np.array(embeddings)\n",
    "\n",
    "# Only embed sentences with a confident topic label\n",
    "mask = results_df['label'].notnull()\n",
    "embed_sentences = results_df.loc[mask, 'sentence'].tolist()\n",
    "sentence_embeddings = bertje_embed(embed_sentences, tokenizer, embedder, device=device, batch_size=32)\n",
    "\n",
    "# Store embeddings back in the DataFrame (as lists), else None\n",
    "embedding_col = [None] * len(results_df)\n",
    "for i, idx in enumerate(results_df.index[mask]):\n",
    "    embedding_col[idx] = sentence_embeddings[i].tolist()\n",
    "results_df['embedding'] = embedding_col\n",
    "\n",
    "# Save as CSV and pickle\n",
    "results_df.to_csv('policy-bertje-matched_results-2015.csv', index=False)\n",
    "with open(\"policy-bertje-matched_results-2015.pickle\", \"wb\") as f:\n",
    "    pickle.dump(results_df, f)\n",
    "\n",
    "print(results_df.head())\n",
    "print(results_df['topic'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3fca7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import defaultdict\n",
    "\n",
    "# Load dictionary\n",
    "dictionary_df = pd.read_excel('policy_dictionairy.xlsx')\n",
    "\n",
    "# Load training_df\n",
    "train_df = pd.read_excel('policy-regex-matched_sentences.csv', index=False)\n",
    "\n",
    "# Load policy documents\n",
    "unseen_policy_df = pd.read_excel(\"Policy-documents/2015_selectedtypes_cleaned (1).xlsx\")\n",
    "nlp = spacy.load(\"nl_core_news_sm\")\n",
    "\n",
    "# Load topic mappings\n",
    "with open('policy-label2topic.pickle', 'rb') as fp:\n",
    "    label2topic = pickle.load(fp)\n",
    "with open('policy-topic2label.pickle', 'rb') as fp:\n",
    "    topic2label = pickle.load(fp)\n",
    "\n",
    "display(label2topic)\n",
    "\n",
    "# Device detection\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "MODEL_DIR = \"bertje_policy/checkpoint-63\"\n",
    "BASE_MODEL = \"GroNLP/bert-base-dutch-cased\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_DIR).to(device)\n",
    "embedder = AutoModel.from_pretrained(BASE_MODEL).to(device)\n",
    "\n",
    "def split_sentences(text):\n",
    "    doc = nlp(str(text))\n",
    "    return [sent.text.strip() for sent in doc.sents]\n",
    "\n",
    "# DataFrame build\n",
    "results_df = pd.DataFrame({\n",
    "    'document': doc_ids,\n",
    "    'sentence': all_sentences,\n",
    "    'labels' : [r['label'] for r in results],\n",
    "    'scores': [r['score'] for r in results]\n",
    "})\n",
    "\n",
    "\n",
    "# STEP 1: Group training sentences by topic name using label2topic\n",
    "topic_sentences = defaultdict(list)\n",
    "for i, row in train_df.iterrows():\n",
    "    if pd.notnull(row['sentence']) and pd.notnull(row['label_int']):\n",
    "        topic = label2topic[row['label_int']]\n",
    "        topic_sentences[topic].append(row['sentence'])\n",
    "\n",
    "# STEP 2: Embed topic sentences with fine-tuned embedder\n",
    "def bertje_embed(sentences, tokenizer, model, device, batch_size=32):\n",
    "    embeddings = []\n",
    "    for i in range(0, len(sentences), batch_size):\n",
    "        batch = sentences[i:i+batch_size]\n",
    "        encoded = tokenizer(batch, return_tensors='pt', padding=True, truncation=True, max_length=128).to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(**encoded)\n",
    "        batch_embeds = output.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "        embeddings.extend(batch_embeds)\n",
    "    return np.array(embeddings)\n",
    "\n",
    "topic_centroids = {}\n",
    "for topic, sents in topic_sentences.items():\n",
    "    embeds = bertje_embed(sents, tokenizer, embedder, device=device, batch_size=32)\n",
    "    topic_centroids[topic] = np.mean(embeds, axis=0)\n",
    "\n",
    "# STEP 3: Embed all new policy sentences (from your policy documents)\n",
    "test_embeddings = bertje_embed(all_sentences, tokenizer, embedder, device=device, batch_size=32)\n",
    "\n",
    "# STEP 4: Similarity and topic assignment\n",
    "topic_names = list(topic_centroids.keys())\n",
    "topic_matrix = np.stack([topic_centroids[t] for t in topic_names])\n",
    "\n",
    "similarities = cosine_similarity(test_embeddings, topic_matrix)\n",
    "best_indices = similarities.argmax(axis=1)\n",
    "best_scores = similarities.max(axis=1)\n",
    "labels = [topic_names[idx] if best_scores[i] > 0.65 else None for i, idx in enumerate(best_indices)]\n",
    "\n",
    "# STEP 5: Collect results\n",
    "results_centroid_df = pd.DataFrame({\n",
    "    'document': doc_ids,\n",
    "    'sentence': all_sentences,\n",
    "    'topic_centroid': labels,\n",
    "    'centroid_score': best_scores\n",
    "})\n",
    "\n",
    "print(results_centroid_df.head())\n",
    "print(results_centroid_df['topic_centroid'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "929d6a96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            document  \\\n",
      "0  mariene-strategie-voor-het-nederlandse-deel-va...   \n",
      "1  mariene-strategie-voor-het-nederlandse-deel-va...   \n",
      "2  mariene-strategie-voor-het-nederlandse-deel-va...   \n",
      "3  mariene-strategie-voor-het-nederlandse-deel-va...   \n",
      "4  mariene-strategie-voor-het-nederlandse-deel-va...   \n",
      "\n",
      "                                            sentence      topic_centroid  \\\n",
      "0                                            Mariene         not-matched   \n",
      "1  Strategie voor het Nederlandse deel van de Noo...  buitenlandse_zaken   \n",
      "2                                            Mariene         not-matched   \n",
      "3  Strategie voor het Nederlandse deel van de Noo...  buitenlandse_zaken   \n",
      "4              KRM-programma van maatregelen Bijlage       sociale_zaken   \n",
      "\n",
      "   centroid_score  \n",
      "0        0.766507  \n",
      "1        0.772560  \n",
      "2        0.766507  \n",
      "3        0.772560  \n",
      "4        0.865638  \n",
      "topic_centroid\n",
      "not-matched              25282\n",
      "buitenlandse_zaken       17451\n",
      "sociale_zaken             7085\n",
      "democratie_en_bestuur     1734\n",
      "criminaliteit             1187\n",
      "milieu                    1000\n",
      "onderwijs                  948\n",
      "Ramp_ongeval               710\n",
      "arbeid                     371\n",
      "gezondheidszorg            243\n",
      "immigratie                 239\n",
      "ruimtelijke_ordening       235\n",
      "huisvesting                178\n",
      "burgerrechten              178\n",
      "Cultuur_en_sport           133\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "\n",
    "# Load resources\n",
    "dictionary_df = pd.read_excel('policy_dictionairy.xlsx')\n",
    "train_df = pd.read_csv('Policy_regex_filtered_matched')  # Removed index=False\n",
    "unseen_policy_df = pd.read_excel(\"Policy-documents/2015_selectedtypes_cleaned (1).xlsx\")\n",
    "nlp = spacy.load(\"nl_core_news_sm\")\n",
    "\n",
    "with open('policy-label2topic.pickle', 'rb') as fp:\n",
    "    label2topic = pickle.load(fp)\n",
    "with open('policy-topic2label.pickle', 'rb') as fp:\n",
    "    topic2label = pickle.load(fp)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "MODEL_DIR = \"bertje_policy/checkpoint-63\"\n",
    "BASE_MODEL = \"GroNLP/bert-base-dutch-cased\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_DIR).to(device)\n",
    "embedder = AutoModel.from_pretrained(BASE_MODEL).to(device)\n",
    "\n",
    "def split_sentences(text):\n",
    "    doc = nlp(str(text))\n",
    "    return [sent.text.strip() for sent in doc.sents]\n",
    "\n",
    "# ---- Split policy docs into sentences ----\n",
    "all_sentences = []\n",
    "doc_ids = []\n",
    "for i, row in unseen_policy_df.iterrows():\n",
    "    doc_id = row['filename'] if 'filename' in row else f'doc_{i}'\n",
    "    text = row['clean_text']\n",
    "    if not pd.isna(text):\n",
    "        sents = split_sentences(text)\n",
    "        all_sentences.extend(sents)\n",
    "        doc_ids.extend([doc_id]*len(sents))\n",
    "\n",
    "# ---- Group training sentences by topic ----\n",
    "topic_sentences = defaultdict(list)\n",
    "for i, row in train_df.iterrows():\n",
    "    if pd.notnull(row['sentence']) and pd.notnull(row['label']):\n",
    "        topic = label2topic[row['label']]\n",
    "        topic_sentences[topic].append(row['sentence'])\n",
    "\n",
    "# ---- Embedder function ----\n",
    "def bertje_embed(sentences, tokenizer, model, device, batch_size=32):\n",
    "    embeddings = []\n",
    "    for i in range(0, len(sentences), batch_size):\n",
    "        batch = sentences[i:i+batch_size]\n",
    "        encoded = tokenizer(batch, return_tensors='pt', padding=True, truncation=True, max_length=128).to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(**encoded)\n",
    "        batch_embeds = output.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "        embeddings.extend(batch_embeds)\n",
    "    return np.array(embeddings)\n",
    "\n",
    "# ---- Build centroids ----\n",
    "topic_centroids = {}\n",
    "for topic, sents in topic_sentences.items():\n",
    "    embeds = bertje_embed(sents, tokenizer, embedder, device=device, batch_size=32)\n",
    "    topic_centroids[topic] = np.mean(embeds, axis=0)\n",
    "\n",
    "# ---- Embed all policy sentences ----\n",
    "test_embeddings = bertje_embed(all_sentences, tokenizer, embedder, device=device, batch_size=32)\n",
    "\n",
    "# ---- Similarity and topic assignment ----\n",
    "topic_names = list(topic_centroids.keys())\n",
    "topic_matrix = np.stack([topic_centroids[t] for t in topic_names])\n",
    "\n",
    "similarities = cosine_similarity(test_embeddings, topic_matrix)\n",
    "best_indices = similarities.argmax(axis=1)\n",
    "best_scores = similarities.max(axis=1)\n",
    "labels = [topic_names[idx] if best_scores[i] > 0.65 else None for i, idx in enumerate(best_indices)]\n",
    "\n",
    "# ---- Collect results ----\n",
    "results_centroid_df = pd.DataFrame({\n",
    "    'document': doc_ids,\n",
    "    'sentence': all_sentences,\n",
    "    'topic_centroid': labels,\n",
    "    'centroid_score': best_scores\n",
    "})\n",
    "\n",
    "print(results_centroid_df.head())\n",
    "print(results_centroid_df['topic_centroid'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8af27341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics included for centroid construction: ['criminaliteit', 'arbeid', 'onderwijs', 'Cultuur_en_sport', 'huisvesting', 'immigratie', 'gezondheidszorg', 'Ramp_ongeval', 'burgerrechten']\n",
      "                                            document  \\\n",
      "0  mariene-strategie-voor-het-nederlandse-deel-va...   \n",
      "1  mariene-strategie-voor-het-nederlandse-deel-va...   \n",
      "2  mariene-strategie-voor-het-nederlandse-deel-va...   \n",
      "3  mariene-strategie-voor-het-nederlandse-deel-va...   \n",
      "4  mariene-strategie-voor-het-nederlandse-deel-va...   \n",
      "\n",
      "                                            sentence    topic_centroid  \\\n",
      "0                                            Mariene        immigratie   \n",
      "1  Strategie voor het Nederlandse deel van de Noo...  Cultuur_en_sport   \n",
      "2                                            Mariene        immigratie   \n",
      "3  Strategie voor het Nederlandse deel van de Noo...  Cultuur_en_sport   \n",
      "4              KRM-programma van maatregelen Bijlage     criminaliteit   \n",
      "\n",
      "   centroid_score  \n",
      "0        0.756533  \n",
      "1        0.754716  \n",
      "2        0.756533  \n",
      "3        0.754716  \n",
      "4        0.845325  \n",
      "topic_centroid\n",
      "arbeid              16463\n",
      "criminaliteit       10703\n",
      "gezondheidszorg      8916\n",
      "onderwijs            7990\n",
      "immigratie           4647\n",
      "Ramp_ongeval         2551\n",
      "huisvesting          2547\n",
      "Cultuur_en_sport     1477\n",
      "burgerrechten        1283\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "\n",
    "# --- LOAD RESOURCES ---\n",
    "dictionary_df = pd.read_excel('policy_dictionairy.xlsx')\n",
    "train_df = pd.read_csv('Policy_regex_filtered_matched')  # Use read_csv for .csv\n",
    "unseen_policy_df = pd.read_excel(\"Policy-documents/2015_selectedtypes_cleaned (1).xlsx\")\n",
    "nlp = spacy.load(\"nl_core_news_sm\")\n",
    "\n",
    "with open('policy-label2topic.pickle', 'rb') as fp:\n",
    "    label2topic = pickle.load(fp)\n",
    "with open('policy-topic2label.pickle', 'rb') as fp:\n",
    "    topic2label = pickle.load(fp)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "MODEL_DIR = \"bertje_policy/checkpoint-63\"\n",
    "BASE_MODEL = \"GroNLP/bert-base-dutch-cased\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_DIR).to(device)\n",
    "embedder = AutoModel.from_pretrained(BASE_MODEL).to(device)\n",
    "\n",
    "def split_sentences(text):\n",
    "    doc = nlp(str(text))\n",
    "    return [sent.text.strip() for sent in doc.sents]\n",
    "\n",
    "# --- BUILD SENTENCE LISTS FROM POLICY DOCUMENTS ---\n",
    "all_sentences = []\n",
    "doc_ids = []\n",
    "for i, row in unseen_policy_df.iterrows():\n",
    "    doc_id = row['filename'] if 'filename' in row else f'doc_{i}'\n",
    "    text = row['clean_text']\n",
    "    if not pd.isna(text):\n",
    "        sents = split_sentences(text)\n",
    "        all_sentences.extend(sents)\n",
    "        doc_ids.extend([doc_id]*len(sents))\n",
    "\n",
    "\n",
    "# --- GROUP TRAINING SENTENCES BY TOPIC ---\n",
    "topic_sentences = defaultdict(list)\n",
    "for i, row in train_df.iterrows():\n",
    "    if pd.notnull(row['sentence']) and pd.notnull(row['label']):\n",
    "        topic = label2topic[row['label']]\n",
    "        topic_sentences[topic].append(row['sentence'])\n",
    "\n",
    "# --- OPTIONALLY EXCLUDE TOPICS BEFORE EMBEDDING ---\n",
    "exclude_topics = ['not-matched', 'buitenlandse_zaken','democratie_en_bestuur','milieu','sociale_zaken','ruimtelijke_ordening' ]  # <--- EDIT THIS LIST AS NEEDED\n",
    "for excl in exclude_topics:\n",
    "    if excl in topic_sentences:\n",
    "        del topic_sentences[excl]\n",
    "\n",
    "print(\"Topics included for centroid construction:\", list(topic_sentences.keys()))\n",
    "\n",
    "# --- EMBEDDER FUNCTION ---\n",
    "def bertje_embed(sentences, tokenizer, model, device, batch_size=32):\n",
    "    embeddings = []\n",
    "    for i in range(0, len(sentences), batch_size):\n",
    "        batch = sentences[i:i+batch_size]\n",
    "        encoded = tokenizer(batch, return_tensors='pt', padding=True, truncation=True, max_length=128).to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(**encoded)\n",
    "        batch_embeds = output.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "        embeddings.extend(batch_embeds)\n",
    "    return np.array(embeddings)\n",
    "\n",
    "# --- BUILD CENTROIDS ONLY FOR INCLUDED TOPICS ---\n",
    "topic_centroids = {}\n",
    "for topic, sents in topic_sentences.items():\n",
    "    embeds = bertje_embed(sents, tokenizer, embedder, device=device, batch_size=32)\n",
    "    topic_centroids[topic] = np.mean(embeds, axis=0)\n",
    "\n",
    "# --- EMBED ALL POLICY SENTENCES ---\n",
    "test_embeddings = bertje_embed(all_sentences, tokenizer, embedder, device=device, batch_size=32)\n",
    "\n",
    "# --- SIMILARITY AND TOPIC ASSIGNMENT ---\n",
    "topic_names = list(topic_centroids.keys())\n",
    "topic_matrix = np.stack([topic_centroids[t] for t in topic_names])\n",
    "\n",
    "similarities = cosine_similarity(test_embeddings, topic_matrix)\n",
    "best_indices = similarities.argmax(axis=1)\n",
    "best_scores = similarities.max(axis=1)\n",
    "labels = [topic_names[idx] if best_scores[i] > 0.65 else None for i, idx in enumerate(best_indices)]\n",
    "\n",
    "# --- COLLECT RESULTS ---\n",
    "results_centroid_df = pd.DataFrame({\n",
    "    'document': doc_ids,\n",
    "    'sentence': all_sentences,\n",
    "    'topic_centroid': labels,\n",
    "    'centroid_score': best_scores\n",
    "})\n",
    "\n",
    "print(results_centroid_df.head())\n",
    "print(results_centroid_df['topic_centroid'].value_counts())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "72b99394",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_centroid_df.to_csv('Policy_2015_centroid.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0236f8ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            document  \\\n",
      "0  mariene-strategie-voor-het-nederlandse-deel-va...   \n",
      "1  mariene-strategie-voor-het-nederlandse-deel-va...   \n",
      "2  mariene-strategie-voor-het-nederlandse-deel-va...   \n",
      "3  mariene-strategie-voor-het-nederlandse-deel-va...   \n",
      "4  mariene-strategie-voor-het-nederlandse-deel-va...   \n",
      "\n",
      "                                            sentence     label     score  \\\n",
      "0                                            Mariene  LABEL_12  0.118065   \n",
      "1  Strategie voor het Nederlandse deel van de Noo...   LABEL_3  0.167812   \n",
      "2                                            Mariene  LABEL_12  0.118065   \n",
      "3  Strategie voor het Nederlandse deel van de Noo...   LABEL_3  0.167812   \n",
      "4              KRM-programma van maatregelen Bijlage   LABEL_3  0.234807   \n",
      "\n",
      "   label_int               topic  \\\n",
      "0         12           onderwijs   \n",
      "1          3  buitenlandse_zaken   \n",
      "2         12           onderwijs   \n",
      "3          3  buitenlandse_zaken   \n",
      "4          3  buitenlandse_zaken   \n",
      "\n",
      "                                           embedding  \n",
      "0  [-0.012886330485343933, -0.18227195739746094, ...  \n",
      "1  [0.5770301222801208, -0.16932857036590576, -0....  \n",
      "2  [-0.012886330485343933, -0.18227195739746094, ...  \n",
      "3  [0.5770301222801208, -0.16932857036590576, -0....  \n",
      "4  [0.5225617289543152, -0.26490160822868347, -0....  \n",
      "topic\n",
      "buitenlandse_zaken    43416\n",
      "not-matched            7816\n",
      "onderwijs              4240\n",
      "gezondheidszorg        1691\n",
      "arbeid                  663\n",
      "criminaliteit           350\n",
      "immigratie              174\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(results_df.head())\n",
    "print(results_df['topic'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce93af5e",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "try:\n",
    "    print(results_df)\n",
    "except NameError:\n",
    "    with open(\"slavery-bertje-matched_results-2015.pickle\", \"rb\") as f:\n",
    "        results_df = pickle.load(fp)\n",
    "\n",
    "\n",
    "grouped = results_df.groupby('topic')\n",
    "\n",
    "topic_cohesion = {}\n",
    "for topic, group in grouped:\n",
    "    embeds = np.vstack(group['embedding'].values)\n",
    "    if embeds.shape[0] > 1:  # Only if there's more than one sentence\n",
    "        sims = cosine_similarity(embeds,dense_output=True)\n",
    "        # Take mean of upper triangle (excluding the diagonal)\n",
    "        mean_sim = np.mean(sims[np.triu_indices_from(sims, k=1)])\n",
    "        topic_cohesion[topic] = mean_sim\n",
    "    else:\n",
    "        topic_cohesion[topic] = np.nan  # Or 0\n",
    "\n",
    "\n",
    "\n",
    "cohesion_df = pd.DataFrame(\n",
    "    list(topic_cohesion.items()), columns=['topic', 'mean_cosine_similarity']\n",
    ").sort_values('mean_cosine_similarity', ascending=True)\n",
    "\n",
    "all_topics = set(label2topic.values())\n",
    "existing_topics = set(cohesion_df['topic'])\n",
    "missing_topics = all_topics - existing_topics\n",
    "\n",
    "print(cohesion_df)\n",
    "\n",
    "print(results_df['topic'].value_counts())\n",
    "\n",
    "print(results_df['label'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191cf8c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>score</th>\n",
       "      <th>label_int</th>\n",
       "      <th>topic</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mariene-strategie-voor-het-nederlandse-deel-va...</td>\n",
       "      <td>Mariene</td>\n",
       "      <td>LABEL_12</td>\n",
       "      <td>0.118065</td>\n",
       "      <td>12</td>\n",
       "      <td>onderwijs</td>\n",
       "      <td>[-0.012886330485343933, -0.18227195739746094, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mariene-strategie-voor-het-nederlandse-deel-va...</td>\n",
       "      <td>Strategie voor het Nederlandse deel van de Noo...</td>\n",
       "      <td>LABEL_3</td>\n",
       "      <td>0.167812</td>\n",
       "      <td>3</td>\n",
       "      <td>buitenlandse_zaken</td>\n",
       "      <td>[0.5770301222801208, -0.16932857036590576, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mariene-strategie-voor-het-nederlandse-deel-va...</td>\n",
       "      <td>Mariene</td>\n",
       "      <td>LABEL_12</td>\n",
       "      <td>0.118065</td>\n",
       "      <td>12</td>\n",
       "      <td>onderwijs</td>\n",
       "      <td>[-0.012886330485343933, -0.18227195739746094, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mariene-strategie-voor-het-nederlandse-deel-va...</td>\n",
       "      <td>Strategie voor het Nederlandse deel van de Noo...</td>\n",
       "      <td>LABEL_3</td>\n",
       "      <td>0.167812</td>\n",
       "      <td>3</td>\n",
       "      <td>buitenlandse_zaken</td>\n",
       "      <td>[0.5770301222801208, -0.16932857036590576, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mariene-strategie-voor-het-nederlandse-deel-va...</td>\n",
       "      <td>KRM-programma van maatregelen Bijlage</td>\n",
       "      <td>LABEL_3</td>\n",
       "      <td>0.234807</td>\n",
       "      <td>3</td>\n",
       "      <td>buitenlandse_zaken</td>\n",
       "      <td>[0.5225617289543152, -0.26490160822868347, -0....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            document  \\\n",
       "0  mariene-strategie-voor-het-nederlandse-deel-va...   \n",
       "1  mariene-strategie-voor-het-nederlandse-deel-va...   \n",
       "2  mariene-strategie-voor-het-nederlandse-deel-va...   \n",
       "3  mariene-strategie-voor-het-nederlandse-deel-va...   \n",
       "4  mariene-strategie-voor-het-nederlandse-deel-va...   \n",
       "\n",
       "                                            sentence     label     score  \\\n",
       "0                                            Mariene  LABEL_12  0.118065   \n",
       "1  Strategie voor het Nederlandse deel van de Noo...   LABEL_3  0.167812   \n",
       "2                                            Mariene  LABEL_12  0.118065   \n",
       "3  Strategie voor het Nederlandse deel van de Noo...   LABEL_3  0.167812   \n",
       "4              KRM-programma van maatregelen Bijlage   LABEL_3  0.234807   \n",
       "\n",
       "   label_int               topic  \\\n",
       "0         12           onderwijs   \n",
       "1          3  buitenlandse_zaken   \n",
       "2         12           onderwijs   \n",
       "3          3  buitenlandse_zaken   \n",
       "4          3  buitenlandse_zaken   \n",
       "\n",
       "                                           embedding  \n",
       "0  [-0.012886330485343933, -0.18227195739746094, ...  \n",
       "1  [0.5770301222801208, -0.16932857036590576, -0....  \n",
       "2  [-0.012886330485343933, -0.18227195739746094, ...  \n",
       "3  [0.5770301222801208, -0.16932857036590576, -0....  \n",
       "4  [0.5225617289543152, -0.26490160822868347, -0....  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Ensure 'label_int' is numeric\n",
    "results_df['label_int'] = pd.to_numeric(results_df['label_int'], errors='coerce')\n",
    "\n",
    "# Drop rows where label_int is null or 3\n",
    "filtered_df = results_df[~(results_df['label_int'].isnull() | (results_df['label_int'] == 11))].copy()\n",
    "\n",
    "display(filtered_df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b29543",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Ensure 'label_int' is numeric\n",
    "results_df['label_int'] = pd.to_numeric(results_df['label_int'], errors='coerce')\n",
    "\n",
    "# Drop rows where label_int is null or 3\n",
    "filtered_df = results_df[~(results_df['label_int'].isnull() | (results_df['label_int'] == 3))].copy()\n",
    "\n",
    "display(filtered_df.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "policy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
