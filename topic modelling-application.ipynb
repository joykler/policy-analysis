{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5093a966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (2.5.1+cu121)\n",
      "Requirement already satisfied: transformers in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (4.52.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (2.0.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (3.10.0)\n",
      "Requirement already satisfied: plotly in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (6.1.0)\n",
      "Requirement already satisfied: spacy in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (3.8.6)\n",
      "Requirement already satisfied: filelock in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from torch) (80.7.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from transformers) (0.31.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from matplotlib) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from matplotlib) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: narwhals>=1.15.1 in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from plotly) (1.40.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from spacy) (1.0.12)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from spacy) (8.3.6)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from spacy) (0.9.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from spacy) (2.10.3)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from requests->transformers) (2025.4.26)\n",
      "Requirement already satisfied: blis<1.4.0,>=1.3.0 in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.1)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.0)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "^C\n",
      "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
      "Requirement already satisfied: torch in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (2.5.1+cu121)\n",
      "Requirement already satisfied: torchvision in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (0.20.1+cu121)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (2.5.1+cu121)\n",
      "Requirement already satisfied: filelock in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from torch) (80.7.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from torchvision) (2.0.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from torchvision) (11.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\home\\anaconda3\\envs\\policy\\lib\\site-packages (from jinja2->torch) (3.0.2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nl-core-news-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/nl_core_news_sm-3.8.0/nl_core_news_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     - -------------------------------------- 0.5/12.8 MB 2.8 MB/s eta 0:00:05\n",
      "     -- ------------------------------------- 0.8/12.8 MB 2.6 MB/s eta 0:00:05\n",
      "     ---- ----------------------------------- 1.3/12.8 MB 2.2 MB/s eta 0:00:06\n",
      "     ----- ---------------------------------- 1.8/12.8 MB 2.3 MB/s eta 0:00:05\n",
      "     ------- -------------------------------- 2.4/12.8 MB 2.4 MB/s eta 0:00:05\n",
      "     -------- ------------------------------- 2.9/12.8 MB 2.5 MB/s eta 0:00:05\n",
      "     -------- ------------------------------- 2.9/12.8 MB 2.5 MB/s eta 0:00:05\n",
      "     ---------- ----------------------------- 3.4/12.8 MB 2.0 MB/s eta 0:00:05\n",
      "     ------------ --------------------------- 3.9/12.8 MB 2.1 MB/s eta 0:00:05\n",
      "     ------------- -------------------------- 4.5/12.8 MB 2.2 MB/s eta 0:00:04\n",
      "     --------------- ------------------------ 5.0/12.8 MB 2.2 MB/s eta 0:00:04\n",
      "     ----------------- ---------------------- 5.5/12.8 MB 2.2 MB/s eta 0:00:04\n",
      "     ------------------ --------------------- 6.0/12.8 MB 2.3 MB/s eta 0:00:04\n",
      "     -------------------- ------------------- 6.6/12.8 MB 2.3 MB/s eta 0:00:03\n",
      "     ---------------------- ----------------- 7.1/12.8 MB 2.3 MB/s eta 0:00:03\n",
      "     ----------------------- ---------------- 7.6/12.8 MB 2.4 MB/s eta 0:00:03\n",
      "     ------------------------- -------------- 8.1/12.8 MB 2.4 MB/s eta 0:00:02\n",
      "     --------------------------- ------------ 8.9/12.8 MB 2.4 MB/s eta 0:00:02\n",
      "     ----------------------------- ---------- 9.4/12.8 MB 2.5 MB/s eta 0:00:02\n",
      "     ------------------------------- -------- 10.0/12.8 MB 2.5 MB/s eta 0:00:02\n",
      "     -------------------------------- ------- 10.5/12.8 MB 2.5 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 11.0/12.8 MB 2.5 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 11.5/12.8 MB 2.5 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 12.1/12.8 MB 2.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.6/12.8 MB 2.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 12.8/12.8 MB 2.5 MB/s eta 0:00:00\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('nl_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "!pip install torch transformers scikit-learn pandas numpy matplotlib plotly spacy\n",
    "!python -m spacy download nl_core_news_sm\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fec0da8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{np.int64(0): 'Historical_colonialism/Slavery',\n",
       " np.int64(1): 'Legacyof_colonialism/Slavery',\n",
       " np.int64(2): 'Legacyof_inequality/Racism',\n",
       " np.int64(3): 'not-matched'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 67\u001b[39m\n\u001b[32m     59\u001b[39m nlp_pipe = pipeline(\n\u001b[32m     60\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtext-classification\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     61\u001b[39m     model=model,\n\u001b[32m     62\u001b[39m     tokenizer=tokenizer,\n\u001b[32m     63\u001b[39m     device=\u001b[32m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m -\u001b[32m1\u001b[39m,\n\u001b[32m     64\u001b[39m )\n\u001b[32m     66\u001b[39m \u001b[38;5;66;03m# Predict topics (batched)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m results = nlp_pipe(all_sentences, truncation=\u001b[38;5;28;01mTrue\u001b[39;00m, max_length=\u001b[32m128\u001b[39m, batch_size=\u001b[32m32\u001b[39m)\n\u001b[32m     69\u001b[39m \u001b[38;5;66;03m# filtered_results = []\u001b[39;00m\n\u001b[32m     70\u001b[39m \u001b[38;5;66;03m# for r in results:\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[38;5;66;03m#     if(r['score'] >= 0.7):\u001b[39;00m\n\u001b[32m     72\u001b[39m \u001b[38;5;66;03m#         filtered_results.append(r)\u001b[39;00m\n\u001b[32m     73\u001b[39m \n\u001b[32m     74\u001b[39m \u001b[38;5;66;03m# Build DataFrame with all columns aligned\u001b[39;00m\n\u001b[32m     75\u001b[39m results_df = pd.DataFrame({\n\u001b[32m     76\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mdocument\u001b[39m\u001b[33m'\u001b[39m: doc_ids,\n\u001b[32m     77\u001b[39m     \u001b[33m'\u001b[39m\u001b[33msentence\u001b[39m\u001b[33m'\u001b[39m: all_sentences,\n\u001b[32m     78\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m'\u001b[39m: [r[\u001b[33m'\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m results],\n\u001b[32m     79\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mscore\u001b[39m\u001b[33m'\u001b[39m: [r[\u001b[33m'\u001b[39m\u001b[33mscore\u001b[39m\u001b[33m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m results]\n\u001b[32m     80\u001b[39m })\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Home\\anaconda3\\envs\\policy\\Lib\\site-packages\\transformers\\pipelines\\text_classification.py:159\u001b[39m, in \u001b[36mTextClassificationPipeline.__call__\u001b[39m\u001b[34m(self, inputs, **kwargs)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    125\u001b[39m \u001b[33;03mClassify the text(s) given as inputs.\u001b[39;00m\n\u001b[32m    126\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    156\u001b[39m \u001b[33;03m    If `top_k` is used, one such dictionary is returned per label.\u001b[39;00m\n\u001b[32m    157\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    158\u001b[39m inputs = (inputs,)\n\u001b[32m--> \u001b[39m\u001b[32m159\u001b[39m result = \u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m(*inputs, **kwargs)\n\u001b[32m    160\u001b[39m \u001b[38;5;66;03m# TODO try and retrieve it in a nicer way from _sanitize_parameters.\u001b[39;00m\n\u001b[32m    161\u001b[39m _legacy = \u001b[33m\"\u001b[39m\u001b[33mtop_k\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Home\\anaconda3\\envs\\policy\\Lib\\site-packages\\transformers\\pipelines\\base.py:1412\u001b[39m, in \u001b[36mPipeline.__call__\u001b[39m\u001b[34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[39m\n\u001b[32m   1408\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m can_use_iterator:\n\u001b[32m   1409\u001b[39m     final_iterator = \u001b[38;5;28mself\u001b[39m.get_iterator(\n\u001b[32m   1410\u001b[39m         inputs, num_workers, batch_size, preprocess_params, forward_params, postprocess_params\n\u001b[32m   1411\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1412\u001b[39m     outputs = \u001b[38;5;28mlist\u001b[39m(final_iterator)\n\u001b[32m   1413\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n\u001b[32m   1414\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Home\\anaconda3\\envs\\policy\\Lib\\site-packages\\transformers\\pipelines\\pt_utils.py:125\u001b[39m, in \u001b[36mPipelineIterator.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    123\u001b[39m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[32m    124\u001b[39m item = \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m.iterator)\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m processed = \u001b[38;5;28mself\u001b[39m.infer(item, **\u001b[38;5;28mself\u001b[39m.params)\n\u001b[32m    126\u001b[39m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.loader_batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    128\u001b[39m     \u001b[38;5;66;03m# Try to infer the size of the batch\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline, AutoModel\n",
    "import pickle\n",
    "\n",
    "\n",
    "# Load dictionary\n",
    "dictionary_df=pd.read_excel('slavery_dictionairy.xlsx')\n",
    "\n",
    "# Load policy documents\n",
    "policy_df = pd.read_excel('Policy-documents/2015_selectedtypes_cleaned (1).xlsx')\n",
    "\n",
    "\n",
    "\n",
    "with open('slavery-label2topic.pickle', 'rb') as fp:\n",
    "    label2topic = pickle.load(fp)\n",
    "\n",
    "with open('slavery-topic2label.pickle', 'rb') as fp:\n",
    "    topic2label = pickle.load(fp)\n",
    "\n",
    "\n",
    "display(label2topic)\n",
    "\n",
    "\n",
    "\n",
    "# Device detection\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "MODEL_DIR = \"bertje_slavery/checkpoint-1278\"\n",
    "BASE_MODEL = \"GroNLP/bert-base-dutch-cased\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_DIR).to(device)\n",
    "embedder = AutoModel.from_pretrained(BASE_MODEL).to(device)\n",
    "\n",
    "unseen_policy_df = pd.read_excel(\"Policy-documents/2015_selectedtypes_cleaned (1).xlsx\")\n",
    "nlp = spacy.load(\"nl_core_news_sm\")\n",
    "\n",
    "def split_sentences(text):\n",
    "    doc = nlp(str(text))\n",
    "    return [sent.text.strip() for sent in doc.sents]\n",
    "\n",
    "# Prepare all lists in a single loop!\n",
    "all_sentences = []\n",
    "doc_ids = []\n",
    "\n",
    "for i, row in unseen_policy_df.iterrows():\n",
    "    doc_id = row['filename'] if 'filename' in row else f'doc_{i}'\n",
    "    text = row['clean_text']\n",
    "    if not pd.isna(text):\n",
    "        sents = split_sentences(text)\n",
    "        all_sentences.extend(sents)\n",
    "        doc_ids.extend([doc_id]*len(sents))\n",
    "\n",
    "# HuggingFace pipeline for topic prediction\n",
    "nlp_pipe = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=0 if torch.cuda.is_available() else -1,\n",
    ")\n",
    "\n",
    "# Predict topics (batched)\n",
    "results = nlp_pipe(all_sentences, truncation=True, max_length=128, batch_size=32)\n",
    "\n",
    "# filtered_results = []\n",
    "# for r in results:\n",
    "#     if(r['score'] >= 0.7):\n",
    "#         filtered_results.append(r)\n",
    "\n",
    "# Build DataFrame with all columns aligned\n",
    "results_df = pd.DataFrame({\n",
    "    'document': doc_ids,\n",
    "    'sentence': all_sentences,\n",
    "    'label': [r['label'] for r in results],\n",
    "    'score': [r['score'] for r in results]\n",
    "})\n",
    "\n",
    "results_df['label_int'] = results_df['label'].apply(lambda x: int(x.replace('LABEL_', '')))\n",
    "results_df['topic'] = results_df['label_int'].map(label2topic)\n",
    "\n",
    "# Embedding function\n",
    "def bertje_embed(sentences, tokenizer, model, device=device, batch_size=32):\n",
    "    embeddings = []\n",
    "    for i in range(0, len(sentences), batch_size):\n",
    "        batch = sentences[i:i+batch_size]\n",
    "        encoded = tokenizer(batch, return_tensors='pt', padding=True, truncation=True, max_length=128).to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(**encoded)\n",
    "        batch_embeds = output.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "        embeddings.extend(batch_embeds)\n",
    "    return np.array(embeddings)\n",
    "\n",
    "# Generate embeddings\n",
    "sentence_embeddings = bertje_embed(results_df['sentence'].tolist(), tokenizer, embedder, device=device, batch_size=32)\n",
    "results_df['embedding'] = [emb.tolist() for emb in sentence_embeddings]\n",
    "\n",
    "results_df.to_csv('slavery-bertje-matched_results-2015.csv', index=False)\n",
    "\n",
    "print(results_df.head())\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open(\"slavery-bertje-matched_results-2015.pickle\", \"wb\") as f:\n",
    "    pickle.dump(results_df, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "57e3525c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>score</th>\n",
       "      <th>label_int</th>\n",
       "      <th>topic</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [document, sentence, label, score, label_int, topic, embedding]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "filtered_df = results_df [results_df['label_int'] < 3]\n",
    "display(filtered_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92be18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Drop rows with missing label_int before filtering\n",
    "filtered_df = results_df[results_df['label_int'].notnull() & (results_df['label_int'] < 3)]\n",
    "\n",
    "display(filtered_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4d1af545",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{np.int64(0): 'Cultuur_en_sport',\n",
       " np.int64(1): 'Ramp_ongeval',\n",
       " np.int64(2): 'arbeid',\n",
       " np.int64(3): 'buitenlandse_zaken',\n",
       " np.int64(4): 'burgerrechten',\n",
       " np.int64(5): 'criminaliteit',\n",
       " np.int64(6): 'democratie_en_bestuur',\n",
       " np.int64(7): 'gezondheidszorg',\n",
       " np.int64(8): 'huisvesting',\n",
       " np.int64(9): 'immigratie',\n",
       " np.int64(10): 'milieu',\n",
       " np.int64(11): 'not-matched',\n",
       " np.int64(12): 'onderwijs',\n",
       " np.int64(13): 'religie',\n",
       " np.int64(14): 'ruimtelijke_ordening',\n",
       " np.int64(15): 'sociale_zaken'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            document  \\\n",
      "0  mariene-strategie-voor-het-nederlandse-deel-va...   \n",
      "1  mariene-strategie-voor-het-nederlandse-deel-va...   \n",
      "2  mariene-strategie-voor-het-nederlandse-deel-va...   \n",
      "3  mariene-strategie-voor-het-nederlandse-deel-va...   \n",
      "4  mariene-strategie-voor-het-nederlandse-deel-va...   \n",
      "\n",
      "                                            sentence     label     score  \\\n",
      "0                                            Mariene  LABEL_12  0.118065   \n",
      "1  Strategie voor het Nederlandse deel van de Noo...   LABEL_3  0.167812   \n",
      "2                                            Mariene  LABEL_12  0.118065   \n",
      "3  Strategie voor het Nederlandse deel van de Noo...   LABEL_3  0.167812   \n",
      "4              KRM-programma van maatregelen Bijlage   LABEL_3  0.234807   \n",
      "\n",
      "   label_int               topic  \\\n",
      "0         12           onderwijs   \n",
      "1          3  buitenlandse_zaken   \n",
      "2         12           onderwijs   \n",
      "3          3  buitenlandse_zaken   \n",
      "4          3  buitenlandse_zaken   \n",
      "\n",
      "                                           embedding  \n",
      "0  [-0.012886330485343933, -0.18227195739746094, ...  \n",
      "1  [0.5770301222801208, -0.16932857036590576, -0....  \n",
      "2  [-0.012886330485343933, -0.18227195739746094, ...  \n",
      "3  [0.5770301222801208, -0.16932857036590576, -0....  \n",
      "4  [0.5225617289543152, -0.26490160822868347, -0....  \n",
      "topic\n",
      "buitenlandse_zaken    43416\n",
      "not-matched            7816\n",
      "onderwijs              4240\n",
      "gezondheidszorg        1691\n",
      "arbeid                  663\n",
      "criminaliteit           350\n",
      "immigratie              174\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline, AutoModel\n",
    "import pickle\n",
    "\n",
    "# Load dictionary\n",
    "dictionary_df = pd.read_excel('policy_dictionairy.xlsx')\n",
    "\n",
    "# Load policy documents\n",
    "unseen_policy_df = pd.read_excel(\"Policy-documents/2015_selectedtypes_cleaned (1).xlsx\")\n",
    "nlp = spacy.load(\"nl_core_news_sm\")\n",
    "\n",
    "# Load topic mappings\n",
    "with open('policy-label2topic.pickle', 'rb') as fp:\n",
    "    label2topic = pickle.load(fp)\n",
    "with open('policy-topic2label.pickle', 'rb') as fp:\n",
    "    topic2label = pickle.load(fp)\n",
    "\n",
    "display(label2topic)\n",
    "\n",
    "# Device detection\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "MODEL_DIR = \"bertje_policy/checkpoint-63\"\n",
    "BASE_MODEL = \"GroNLP/bert-base-dutch-cased\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_DIR).to(device)\n",
    "embedder = AutoModel.from_pretrained(BASE_MODEL).to(device)\n",
    "\n",
    "def split_sentences(text):\n",
    "    doc = nlp(str(text))\n",
    "    return [sent.text.strip() for sent in doc.sents]\n",
    "\n",
    "# Collect all sentences and doc_ids\n",
    "all_sentences = []\n",
    "doc_ids = []\n",
    "for i, row in unseen_policy_df.iterrows():\n",
    "    doc_id = row['filename'] if 'filename' in row else f'doc_{i}'\n",
    "    text = row['clean_text']\n",
    "    if not pd.isna(text):\n",
    "        sents = split_sentences(text)\n",
    "        all_sentences.extend(sents)\n",
    "        doc_ids.extend([doc_id]*len(sents))\n",
    "\n",
    "# HuggingFace pipeline for topic prediction\n",
    "nlp_pipe = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=0 if torch.cuda.is_available() else -1,\n",
    ")\n",
    "\n",
    "# Predict topics (batched)\n",
    "results = nlp_pipe(all_sentences, truncation=True, max_length=128, batch_size=32)\n",
    "\n",
    "# Keep results and sentences aligned; fill label/score only if score >= 0.7\n",
    "#labels = []\n",
    "#cores = []\n",
    "#for r in results:\n",
    "#    if r['score'] >= 0.3:\n",
    "#        labels.append(r['label'])\n",
    "#        scores.append(r['score'])\n",
    "#    else:\n",
    "#        labels.append(None)\n",
    "#        scores.append(None)\n",
    "\n",
    "labels = [r['label'] for r in results]\n",
    "scores = [r['score'] for r in results]\n",
    "\n",
    "# DataFrame build\n",
    "results_df = pd.DataFrame({\n",
    "    'document': doc_ids,\n",
    "    'sentence': all_sentences,\n",
    "    'label': labels,\n",
    "    'score': scores\n",
    "})\n",
    "\n",
    "# Label int and topic only for confident predictions\n",
    "results_df['label_int'] = results_df['label'].apply(lambda x: int(x.replace('LABEL_', '')) if pd.notnull(x) else None)\n",
    "results_df['topic'] = results_df['label_int'].map(label2topic)\n",
    "\n",
    "# Embedding function (only for sentences with valid label)\n",
    "def bertje_embed(sentences, tokenizer, model, device=device, batch_size=32):\n",
    "    embeddings = []\n",
    "    for i in range(0, len(sentences), batch_size):\n",
    "        batch = sentences[i:i+batch_size]\n",
    "        encoded = tokenizer(batch, return_tensors='pt', padding=True, truncation=True, max_length=128).to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(**encoded)\n",
    "        batch_embeds = output.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "        embeddings.extend(batch_embeds)\n",
    "    return np.array(embeddings)\n",
    "\n",
    "# Only embed sentences with a confident topic label\n",
    "mask = results_df['label'].notnull()\n",
    "embed_sentences = results_df.loc[mask, 'sentence'].tolist()\n",
    "sentence_embeddings = bertje_embed(embed_sentences, tokenizer, embedder, device=device, batch_size=32)\n",
    "\n",
    "# Store embeddings back in the DataFrame (as lists), else None\n",
    "embedding_col = [None] * len(results_df)\n",
    "for i, idx in enumerate(results_df.index[mask]):\n",
    "    embedding_col[idx] = sentence_embeddings[i].tolist()\n",
    "results_df['embedding'] = embedding_col\n",
    "\n",
    "# Save as CSV and pickle\n",
    "results_df.to_csv('policy-bertje-matched_results-2015.csv', index=False)\n",
    "with open(\"policy-bertje-matched_results-2015.pickle\", \"wb\") as f:\n",
    "    pickle.dump(results_df, f)\n",
    "\n",
    "print(results_df.head())\n",
    "print(results_df['topic'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0236f8ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            document  \\\n",
      "0  mariene-strategie-voor-het-nederlandse-deel-va...   \n",
      "1  mariene-strategie-voor-het-nederlandse-deel-va...   \n",
      "2  mariene-strategie-voor-het-nederlandse-deel-va...   \n",
      "3  mariene-strategie-voor-het-nederlandse-deel-va...   \n",
      "4  mariene-strategie-voor-het-nederlandse-deel-va...   \n",
      "\n",
      "                                            sentence     label     score  \\\n",
      "0                                            Mariene  LABEL_12  0.118065   \n",
      "1  Strategie voor het Nederlandse deel van de Noo...   LABEL_3  0.167812   \n",
      "2                                            Mariene  LABEL_12  0.118065   \n",
      "3  Strategie voor het Nederlandse deel van de Noo...   LABEL_3  0.167812   \n",
      "4              KRM-programma van maatregelen Bijlage   LABEL_3  0.234807   \n",
      "\n",
      "   label_int               topic  \\\n",
      "0         12           onderwijs   \n",
      "1          3  buitenlandse_zaken   \n",
      "2         12           onderwijs   \n",
      "3          3  buitenlandse_zaken   \n",
      "4          3  buitenlandse_zaken   \n",
      "\n",
      "                                           embedding  \n",
      "0  [-0.012886330485343933, -0.18227195739746094, ...  \n",
      "1  [0.5770301222801208, -0.16932857036590576, -0....  \n",
      "2  [-0.012886330485343933, -0.18227195739746094, ...  \n",
      "3  [0.5770301222801208, -0.16932857036590576, -0....  \n",
      "4  [0.5225617289543152, -0.26490160822868347, -0....  \n",
      "topic\n",
      "buitenlandse_zaken    43416\n",
      "not-matched            7816\n",
      "onderwijs              4240\n",
      "gezondheidszorg        1691\n",
      "arbeid                  663\n",
      "criminaliteit           350\n",
      "immigratie              174\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(results_df.head())\n",
    "print(results_df['topic'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce93af5e",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "try:\n",
    "    print(results_df)\n",
    "except NameError:\n",
    "    with open(\"slavery-bertje-matched_results-2015.pickle\", \"rb\") as f:\n",
    "        results_df = pickle.load(fp)\n",
    "\n",
    "\n",
    "grouped = results_df.groupby('topic')\n",
    "\n",
    "topic_cohesion = {}\n",
    "for topic, group in grouped:\n",
    "    embeds = np.vstack(group['embedding'].values)\n",
    "    if embeds.shape[0] > 1:  # Only if there's more than one sentence\n",
    "        sims = cosine_similarity(embeds,dense_output=True)\n",
    "        # Take mean of upper triangle (excluding the diagonal)\n",
    "        mean_sim = np.mean(sims[np.triu_indices_from(sims, k=1)])\n",
    "        topic_cohesion[topic] = mean_sim\n",
    "    else:\n",
    "        topic_cohesion[topic] = np.nan  # Or 0\n",
    "\n",
    "\n",
    "\n",
    "cohesion_df = pd.DataFrame(\n",
    "    list(topic_cohesion.items()), columns=['topic', 'mean_cosine_similarity']\n",
    ").sort_values('mean_cosine_similarity', ascending=True)\n",
    "\n",
    "all_topics = set(label2topic.values())\n",
    "existing_topics = set(cohesion_df['topic'])\n",
    "missing_topics = all_topics - existing_topics\n",
    "\n",
    "print(cohesion_df)\n",
    "\n",
    "print(results_df['topic'].value_counts())\n",
    "\n",
    "print(results_df['label'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191cf8c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>score</th>\n",
       "      <th>label_int</th>\n",
       "      <th>topic</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mariene-strategie-voor-het-nederlandse-deel-va...</td>\n",
       "      <td>Mariene</td>\n",
       "      <td>LABEL_12</td>\n",
       "      <td>0.118065</td>\n",
       "      <td>12</td>\n",
       "      <td>onderwijs</td>\n",
       "      <td>[-0.012886330485343933, -0.18227195739746094, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mariene-strategie-voor-het-nederlandse-deel-va...</td>\n",
       "      <td>Strategie voor het Nederlandse deel van de Noo...</td>\n",
       "      <td>LABEL_3</td>\n",
       "      <td>0.167812</td>\n",
       "      <td>3</td>\n",
       "      <td>buitenlandse_zaken</td>\n",
       "      <td>[0.5770301222801208, -0.16932857036590576, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mariene-strategie-voor-het-nederlandse-deel-va...</td>\n",
       "      <td>Mariene</td>\n",
       "      <td>LABEL_12</td>\n",
       "      <td>0.118065</td>\n",
       "      <td>12</td>\n",
       "      <td>onderwijs</td>\n",
       "      <td>[-0.012886330485343933, -0.18227195739746094, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mariene-strategie-voor-het-nederlandse-deel-va...</td>\n",
       "      <td>Strategie voor het Nederlandse deel van de Noo...</td>\n",
       "      <td>LABEL_3</td>\n",
       "      <td>0.167812</td>\n",
       "      <td>3</td>\n",
       "      <td>buitenlandse_zaken</td>\n",
       "      <td>[0.5770301222801208, -0.16932857036590576, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mariene-strategie-voor-het-nederlandse-deel-va...</td>\n",
       "      <td>KRM-programma van maatregelen Bijlage</td>\n",
       "      <td>LABEL_3</td>\n",
       "      <td>0.234807</td>\n",
       "      <td>3</td>\n",
       "      <td>buitenlandse_zaken</td>\n",
       "      <td>[0.5225617289543152, -0.26490160822868347, -0....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            document  \\\n",
       "0  mariene-strategie-voor-het-nederlandse-deel-va...   \n",
       "1  mariene-strategie-voor-het-nederlandse-deel-va...   \n",
       "2  mariene-strategie-voor-het-nederlandse-deel-va...   \n",
       "3  mariene-strategie-voor-het-nederlandse-deel-va...   \n",
       "4  mariene-strategie-voor-het-nederlandse-deel-va...   \n",
       "\n",
       "                                            sentence     label     score  \\\n",
       "0                                            Mariene  LABEL_12  0.118065   \n",
       "1  Strategie voor het Nederlandse deel van de Noo...   LABEL_3  0.167812   \n",
       "2                                            Mariene  LABEL_12  0.118065   \n",
       "3  Strategie voor het Nederlandse deel van de Noo...   LABEL_3  0.167812   \n",
       "4              KRM-programma van maatregelen Bijlage   LABEL_3  0.234807   \n",
       "\n",
       "   label_int               topic  \\\n",
       "0         12           onderwijs   \n",
       "1          3  buitenlandse_zaken   \n",
       "2         12           onderwijs   \n",
       "3          3  buitenlandse_zaken   \n",
       "4          3  buitenlandse_zaken   \n",
       "\n",
       "                                           embedding  \n",
       "0  [-0.012886330485343933, -0.18227195739746094, ...  \n",
       "1  [0.5770301222801208, -0.16932857036590576, -0....  \n",
       "2  [-0.012886330485343933, -0.18227195739746094, ...  \n",
       "3  [0.5770301222801208, -0.16932857036590576, -0....  \n",
       "4  [0.5225617289543152, -0.26490160822868347, -0....  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Ensure 'label_int' is numeric\n",
    "results_df['label_int'] = pd.to_numeric(results_df['label_int'], errors='coerce')\n",
    "\n",
    "# Drop rows where label_int is null or 3\n",
    "filtered_df = results_df[~(results_df['label_int'].isnull() | (results_df['label_int'] == 11))].copy()\n",
    "\n",
    "display(filtered_df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b29543",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Ensure 'label_int' is numeric\n",
    "results_df['label_int'] = pd.to_numeric(results_df['label_int'], errors='coerce')\n",
    "\n",
    "# Drop rows where label_int is null or 3\n",
    "filtered_df = results_df[~(results_df['label_int'].isnull() | (results_df['label_int'] == 3))].copy()\n",
    "\n",
    "display(filtered_df.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "policy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
